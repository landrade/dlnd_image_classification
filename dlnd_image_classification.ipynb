{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [00:34, 4.88MB/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f175c6f1cf8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return (x-x.min())/(x.max()-x.min())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return np.eye(10)[x]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, (None, *image_shape), name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, [None,n_classes], name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32,None, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    shape = x_tensor.get_shape().as_list()\n",
    "    size = (conv_ksize[0], conv_ksize[1], shape[3], conv_num_outputs)\n",
    "    W = tf.Variable(tf.truncated_normal(size, 0, (1/ sqrt(shape[1]*shape[2]*shape[3]))))\n",
    "    B = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    tensor = tf.nn.conv2d(x_tensor, W, strides=[1, conv_strides[0], conv_strides[1], 1], padding='SAME')\n",
    "    tensor = tf.nn.bias_add(tensor, B)\n",
    "    tensor = tf.nn.relu(tensor)\n",
    "    tensor = tf.nn.max_pool(tensor, ksize=[1, pool_ksize[0], pool_ksize[1], 1], strides=[1, pool_strides[0], pool_strides[1], 1], padding='SAME')\n",
    "    return tensor\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.reshape(x_tensor, [-1, x_tensor.shape[1].value * x_tensor.shape[2].value* x_tensor.shape[3].value])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    weight= tf.Variable(tf.truncated_normal([x_tensor.shape[1].value, num_outputs], mean=0.0, stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    \n",
    "    return tf.nn.relu(tf.nn.bias_add(tf.matmul(x_tensor, weight), bias))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    weight= tf.Variable(tf.truncated_normal([x_tensor.shape[1].value, num_outputs], mean=0.0, stddev=0.01))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    return tf.nn.bias_add(tf.matmul(x_tensor, weight), bias)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv1 = conv2d_maxpool(x, 20, [5,5], [1,1], [2,2], [2,2])\n",
    "    conv2 = conv2d_maxpool(conv1, 50, conv_ksize=[3,3], conv_strides=[1,1], pool_ksize=[2,2], pool_strides=[2,2])\n",
    "    conv2 = tf.nn.dropout(conv2, keep_prob)\n",
    "    \n",
    "    conv_out = conv2\n",
    "    \n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    fl0 = flatten(conv_out)\n",
    "    \n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    fl1 = fully_conn(fl0, 100)\n",
    "    fl1 = tf.nn.dropout(fl1, keep_prob)\n",
    "    \n",
    "    fl2 = fully_conn(fl1, 50)\n",
    "    fl2 = tf.nn.dropout(fl2, keep_prob+0.2)\n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    out = output(fl2,10)\n",
    "    \n",
    "    \n",
    "    # TODO: return output\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer,feed_dict={x:feature_batch, y:label_batch, keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    loss = session.run(cost, feed_dict={x:feature_batch, y: label_batch, keep_prob:1.0 })\n",
    "    valid_acc =session.run(accuracy, feed_dict={x:valid_features, y: label_batch, keep_prob:1.0})\n",
    "    print('cost: {}  accuracy: {}'.format(loss, valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 2.239238739013672  Accuracy: 0.20000000298023224\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 2.211514472961426  Accuracy: 0.22500000894069672\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 2.155796766281128  Accuracy: 0.3500000238418579\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 2.1297335624694824  Accuracy: 0.4000000059604645\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 2.075791835784912  Accuracy: 0.42500001192092896\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 1.9625229835510254  Accuracy: 0.375\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 1.889718770980835  Accuracy: 0.42499998211860657\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 1.8559966087341309  Accuracy: 0.30000001192092896\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 1.843057632446289  Accuracy: 0.4000000059604645\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 1.7754380702972412  Accuracy: 0.32499998807907104\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 1.715175747871399  Accuracy: 0.42499998211860657\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 1.6916308403015137  Accuracy: 0.4000000059604645\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 1.6669137477874756  Accuracy: 0.44999998807907104\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 1.5753897428512573  Accuracy: 0.42499998211860657\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 1.5587592124938965  Accuracy: 0.5\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 1.5084843635559082  Accuracy: 0.4749999940395355\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 1.5088047981262207  Accuracy: 0.4749999940395355\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 1.4696986675262451  Accuracy: 0.44999998807907104\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 1.4529482126235962  Accuracy: 0.44999998807907104\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 1.373582363128662  Accuracy: 0.5\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 1.3207844495773315  Accuracy: 0.5\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 1.3325068950653076  Accuracy: 0.5\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 1.286324381828308  Accuracy: 0.5\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 1.2994458675384521  Accuracy: 0.5249999761581421\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 1.2705011367797852  Accuracy: 0.550000011920929\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 1.251343846321106  Accuracy: 0.550000011920929\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 1.1992905139923096  Accuracy: 0.6000000238418579\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 1.1943902969360352  Accuracy: 0.625\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 1.1569370031356812  Accuracy: 0.6499999761581421\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 1.1399272680282593  Accuracy: 0.6000000238418579\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 1.1259804964065552  Accuracy: 0.6000000238418579\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 1.1057608127593994  Accuracy: 0.625\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 1.098978042602539  Accuracy: 0.6000000238418579\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 1.0308362245559692  Accuracy: 0.6499999761581421\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 1.0484285354614258  Accuracy: 0.625\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 1.0033960342407227  Accuracy: 0.6499999761581421\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 0.9690029621124268  Accuracy: 0.6000000238418579\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 0.9778611660003662  Accuracy: 0.6499999761581421\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 1.0019947290420532  Accuracy: 0.6749999523162842\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 0.9414532780647278  Accuracy: 0.675000011920929\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 0.893118143081665  Accuracy: 0.6749999523162842\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 0.8483908772468567  Accuracy: 0.75\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 0.8911939859390259  Accuracy: 0.699999988079071\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 0.8558849096298218  Accuracy: 0.7500000596046448\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 0.8122764229774475  Accuracy: 0.7750000357627869\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 0.775282621383667  Accuracy: 0.7500000596046448\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 0.7818293571472168  Accuracy: 0.800000011920929\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 0.8073991537094116  Accuracy: 0.7750000357627869\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 0.7945533990859985  Accuracy: 0.7750000357627869\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 0.7472583055496216  Accuracy: 0.800000011920929\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 0.7221482992172241  Accuracy: 0.824999988079071\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 0.7814146280288696  Accuracy: 0.7750000357627869\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 0.7637385129928589  Accuracy: 0.800000011920929\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 0.7556652426719666  Accuracy: 0.800000011920929\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 0.6847386956214905  Accuracy: 0.824999988079071\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 0.7213413715362549  Accuracy: 0.824999988079071\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 0.7038637399673462  Accuracy: 0.800000011920929\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 0.69085294008255  Accuracy: 0.800000011920929\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 0.7119207382202148  Accuracy: 0.7250000238418579\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 0.6638465523719788  Accuracy: 0.7750000357627869\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 0.6751818060874939  Accuracy: 0.7750000357627869\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 0.6711660027503967  Accuracy: 0.7749999761581421\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 0.6435989737510681  Accuracy: 0.824999988079071\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 0.6533610820770264  Accuracy: 0.800000011920929\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 0.6386611461639404  Accuracy: 0.8500000238418579\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 0.6195228695869446  Accuracy: 0.824999988079071\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 0.5911693572998047  Accuracy: 0.8500000238418579\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 0.5759345889091492  Accuracy: 0.8499999642372131\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 0.5706595182418823  Accuracy: 0.7750000357627869\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 0.5891447067260742  Accuracy: 0.7750000357627869\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 0.552389919757843  Accuracy: 0.8500000238418579\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 0.5402311086654663  Accuracy: 0.824999988079071\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 0.5448400378227234  Accuracy: 0.875\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 0.5256195068359375  Accuracy: 0.875\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 0.5049573183059692  Accuracy: 0.8500000238418579\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 0.489984393119812  Accuracy: 0.875\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 0.498300164937973  Accuracy: 0.8999999761581421\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 0.4900262951850891  Accuracy: 0.8999999761581421\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 0.46835705637931824  Accuracy: 0.875\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 0.44559574127197266  Accuracy: 0.875\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 0.45505768060684204  Accuracy: 0.875\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 0.44733887910842896  Accuracy: 0.9000000357627869\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 0.48049718141555786  Accuracy: 0.875\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 0.44701701402664185  Accuracy: 0.875\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 0.42475688457489014  Accuracy: 0.8750000596046448\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 0.43797963857650757  Accuracy: 0.875\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 0.426392525434494  Accuracy: 0.9000000357627869\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 0.41158831119537354  Accuracy: 0.925000011920929\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 0.4183524549007416  Accuracy: 0.875\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 0.39732956886291504  Accuracy: 0.8499999642372131\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 0.3977782130241394  Accuracy: 0.925000011920929\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 0.39989930391311646  Accuracy: 0.875\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 0.3911365270614624  Accuracy: 0.9000000357627869\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 0.34205150604248047  Accuracy: 0.925000011920929\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 0.3613910675048828  Accuracy: 0.925000011920929\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 0.3805691599845886  Accuracy: 0.9000000357627869\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 0.4105308949947357  Accuracy: 0.8499999642372131\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 0.37238654494285583  Accuracy: 0.925000011920929\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 0.36918410658836365  Accuracy: 0.925000011920929\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 0.3761412799358368  Accuracy: 0.9000000357627869\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 2.23110032081604  Accuracy: 0.15000000596046448\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss: 2.0811448097229004  Accuracy: 0.22499999403953552\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss: 1.8150386810302734  Accuracy: 0.17500001192092896\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss: 1.8493655920028687  Accuracy: 0.2750000059604645\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss: 1.8605520725250244  Accuracy: 0.3500000238418579\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 2.019744396209717  Accuracy: 0.44999998807907104\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss: 1.9853380918502808  Accuracy: 0.30000001192092896\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss: 1.6398416757583618  Accuracy: 0.375\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss: 1.7518644332885742  Accuracy: 0.25\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss: 1.8215985298156738  Accuracy: 0.375\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 1.916717529296875  Accuracy: 0.42500001192092896\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss: 1.913380742073059  Accuracy: 0.42500001192092896\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss: 1.4368817806243896  Accuracy: 0.42500001192092896\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss: 1.6417381763458252  Accuracy: 0.32500001788139343\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss: 1.7000699043273926  Accuracy: 0.4000000059604645\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 1.7133715152740479  Accuracy: 0.42499998211860657\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss: 1.67940092086792  Accuracy: 0.45000001788139343\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss: 1.3231269121170044  Accuracy: 0.550000011920929\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss: 1.5031896829605103  Accuracy: 0.4000000059604645\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss: 1.5663249492645264  Accuracy: 0.4749999940395355\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 1.7101452350616455  Accuracy: 0.42500001192092896\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss: 1.5743016004562378  Accuracy: 0.42500001192092896\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss: 1.2226723432540894  Accuracy: 0.6000000238418579\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss: 1.486209511756897  Accuracy: 0.5\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss: 1.467037320137024  Accuracy: 0.44999998807907104\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 1.5536984205245972  Accuracy: 0.4749999940395355\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss: 1.5144978761672974  Accuracy: 0.42500001192092896\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss: 1.2242553234100342  Accuracy: 0.6000000238418579\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss: 1.3814189434051514  Accuracy: 0.42500001192092896\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss: 1.4493277072906494  Accuracy: 0.4749999940395355\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 1.4712653160095215  Accuracy: 0.45000001788139343\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss: 1.3827488422393799  Accuracy: 0.5\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss: 1.1559580564498901  Accuracy: 0.6499999761581421\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss: 1.4258947372436523  Accuracy: 0.4750000238418579\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss: 1.3920469284057617  Accuracy: 0.44999998807907104\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 1.4808540344238281  Accuracy: 0.5\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss: 1.3411957025527954  Accuracy: 0.5249999761581421\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss: 1.09787917137146  Accuracy: 0.550000011920929\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss: 1.3426928520202637  Accuracy: 0.44999998807907104\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss: 1.3565239906311035  Accuracy: 0.4749999940395355\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 1.4338445663452148  Accuracy: 0.550000011920929\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss: 1.2871595621109009  Accuracy: 0.5750000476837158\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss: 1.0873887538909912  Accuracy: 0.6499999761581421\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss: 1.3388457298278809  Accuracy: 0.5\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss: 1.2978086471557617  Accuracy: 0.5249999761581421\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 1.4439756870269775  Accuracy: 0.5249999761581421\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss: 1.3849257230758667  Accuracy: 0.5\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss: 1.0836002826690674  Accuracy: 0.625\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss: 1.2903239727020264  Accuracy: 0.6000000238418579\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss: 1.2644927501678467  Accuracy: 0.5\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 1.3972760438919067  Accuracy: 0.4749999940395355\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss: 1.221336007118225  Accuracy: 0.5249999761581421\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss: 1.0579137802124023  Accuracy: 0.625\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss: 1.2623875141143799  Accuracy: 0.42500001192092896\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss: 1.2691011428833008  Accuracy: 0.5249999761581421\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 1.343327522277832  Accuracy: 0.550000011920929\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss: 1.2044663429260254  Accuracy: 0.5250000357627869\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss: 0.9600749015808105  Accuracy: 0.7000000476837158\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss: 1.213534951210022  Accuracy: 0.4750000238418579\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss: 1.209592580795288  Accuracy: 0.574999988079071\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 1.2592591047286987  Accuracy: 0.550000011920929\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss: 1.1696985960006714  Accuracy: 0.5250000357627869\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss: 0.9733403921127319  Accuracy: 0.6000000238418579\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss: 1.2110590934753418  Accuracy: 0.6000000238418579\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss: 1.1639094352722168  Accuracy: 0.625\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 1.2656292915344238  Accuracy: 0.6000000238418579\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss: 1.1498247385025024  Accuracy: 0.5250000357627869\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss: 0.962860107421875  Accuracy: 0.6499999761581421\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss: 1.1424386501312256  Accuracy: 0.6000000238418579\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss: 1.0945703983306885  Accuracy: 0.6750000715255737\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 1.230555772781372  Accuracy: 0.6000000238418579\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss: 1.1145973205566406  Accuracy: 0.5250000357627869\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss: 0.9216179251670837  Accuracy: 0.625\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss: 1.1144943237304688  Accuracy: 0.574999988079071\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss: 1.070365309715271  Accuracy: 0.6000000238418579\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 1.240067958831787  Accuracy: 0.5249999761581421\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss: 1.0623787641525269  Accuracy: 0.5250000357627869\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss: 0.9263936281204224  Accuracy: 0.6499999761581421\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss: 1.1004995107650757  Accuracy: 0.5249999761581421\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss: 1.034615397453308  Accuracy: 0.7250000238418579\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 1.2193970680236816  Accuracy: 0.5750000476837158\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss: 1.1141372919082642  Accuracy: 0.574999988079071\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss: 0.9371082782745361  Accuracy: 0.6499999761581421\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss: 1.068742275238037  Accuracy: 0.6499999761581421\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss: 0.9734494090080261  Accuracy: 0.7250000238418579\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 1.1158758401870728  Accuracy: 0.625\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss: 1.0888922214508057  Accuracy: 0.625\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss: 0.8671084642410278  Accuracy: 0.7000000476837158\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss: 1.0716979503631592  Accuracy: 0.6499999761581421\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss: 0.9723024368286133  Accuracy: 0.7000000476837158\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 1.1063026189804077  Accuracy: 0.625\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss: 0.9935147762298584  Accuracy: 0.550000011920929\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss: 0.8511490821838379  Accuracy: 0.6749999523162842\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss: 1.0412847995758057  Accuracy: 0.6000000238418579\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss: 0.9734756946563721  Accuracy: 0.6500000357627869\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 1.042868971824646  Accuracy: 0.7000000476837158\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss: 1.0307788848876953  Accuracy: 0.625\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss: 0.8848550319671631  Accuracy: 0.7250000238418579\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss: 1.0202441215515137  Accuracy: 0.6749999523162842\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss: 0.9579033851623535  Accuracy: 0.675000011920929\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 1.102536678314209  Accuracy: 0.6499999761581421\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss: 0.9837994575500488  Accuracy: 0.625\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss: 0.832116425037384  Accuracy: 0.7250000238418579\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss: 0.9960418939590454  Accuracy: 0.7000000476837158\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss: 0.9210911393165588  Accuracy: 0.7750000357627869\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 1.0429655313491821  Accuracy: 0.6499999761581421\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss: 1.0270652770996094  Accuracy: 0.550000011920929\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss: 0.8190514445304871  Accuracy: 0.75\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss: 0.9444938898086548  Accuracy: 0.6750000715255737\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss: 0.9368729591369629  Accuracy: 0.7250000238418579\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 1.039150595664978  Accuracy: 0.6749999523162842\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss: 0.9249392151832581  Accuracy: 0.625\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss: 0.7896725535392761  Accuracy: 0.6749999523162842\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss: 0.9269677996635437  Accuracy: 0.675000011920929\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss: 0.873988151550293  Accuracy: 0.8250000476837158\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 0.9583244919776917  Accuracy: 0.7000000476837158\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss: 0.9512051939964294  Accuracy: 0.625\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss: 0.7910996675491333  Accuracy: 0.7250000238418579\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss: 0.9486210346221924  Accuracy: 0.675000011920929\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss: 0.8825958371162415  Accuracy: 0.7749999761581421\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 1.0138028860092163  Accuracy: 0.6499999761581421\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss: 0.9612095355987549  Accuracy: 0.6500000357627869\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss: 0.7776152491569519  Accuracy: 0.7000000476837158\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss: 0.87645024061203  Accuracy: 0.6500000357627869\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss: 0.8664568066596985  Accuracy: 0.7750000357627869\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 0.952670156955719  Accuracy: 0.6750000715255737\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss: 0.8760198354721069  Accuracy: 0.6500000357627869\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss: 0.7460619211196899  Accuracy: 0.7500000596046448\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss: 0.8729841709136963  Accuracy: 0.625\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss: 0.8575835227966309  Accuracy: 0.8250000476837158\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 0.8838459253311157  Accuracy: 0.6750000715255737\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss: 0.8334695100784302  Accuracy: 0.6500000357627869\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss: 0.7118016481399536  Accuracy: 0.75\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss: 0.8705730438232422  Accuracy: 0.675000011920929\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss: 0.783484697341919  Accuracy: 0.800000011920929\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 0.9139881134033203  Accuracy: 0.7000000476837158\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss: 0.820401668548584  Accuracy: 0.675000011920929\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss: 0.7305981516838074  Accuracy: 0.75\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss: 0.8941338062286377  Accuracy: 0.7000000476837158\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss: 0.8252873420715332  Accuracy: 0.75\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 0.8736841678619385  Accuracy: 0.7250000238418579\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss: 0.8691093921661377  Accuracy: 0.675000011920929\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss: 0.7348831295967102  Accuracy: 0.75\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss: 0.8299388885498047  Accuracy: 0.7749999761581421\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss: 0.7947132587432861  Accuracy: 0.8000000715255737\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 0.8744235634803772  Accuracy: 0.7000000476837158\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss: 0.8310083150863647  Accuracy: 0.6750000715255737\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss: 0.6563887596130371  Accuracy: 0.7500000596046448\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss: 0.8170052766799927  Accuracy: 0.75\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss: 0.7439149618148804  Accuracy: 0.8750000596046448\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 0.8486785888671875  Accuracy: 0.7250000238418579\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss: 0.7808666229248047  Accuracy: 0.6749999523162842\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss: 0.6931952238082886  Accuracy: 0.75\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss: 0.8125457763671875  Accuracy: 0.75\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss: 0.770286500453949  Accuracy: 0.824999988079071\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 0.8063017129898071  Accuracy: 0.75\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss: 0.8523920774459839  Accuracy: 0.6000000238418579\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss: 0.6719104051589966  Accuracy: 0.8250000476837158\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss: 0.8580383062362671  Accuracy: 0.75\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss: 0.7523517608642578  Accuracy: 0.8250000476837158\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 0.7924887537956238  Accuracy: 0.7749999761581421\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss: 0.820236086845398  Accuracy: 0.6000000238418579\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss: 0.6889471411705017  Accuracy: 0.75\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss: 0.807335615158081  Accuracy: 0.75\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss: 0.7538562417030334  Accuracy: 0.875\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 0.8301761150360107  Accuracy: 0.7250000238418579\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss: 0.7955083847045898  Accuracy: 0.6500000357627869\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss: 0.6791221499443054  Accuracy: 0.8000000715255737\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss: 0.7320631146430969  Accuracy: 0.7250000238418579\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss: 0.691175639629364  Accuracy: 0.8500000238418579\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 0.8008819818496704  Accuracy: 0.75\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss: 0.799242377281189  Accuracy: 0.7000000476837158\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss: 0.6865039467811584  Accuracy: 0.800000011920929\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss: 0.796400785446167  Accuracy: 0.7250000238418579\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss: 0.7252539396286011  Accuracy: 0.824999988079071\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 0.7610399723052979  Accuracy: 0.75\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss: 0.7959044575691223  Accuracy: 0.6499999761581421\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss: 0.6554377675056458  Accuracy: 0.824999988079071\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss: 0.761343240737915  Accuracy: 0.7250000238418579\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss: 0.6772751808166504  Accuracy: 0.875\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 0.7543923854827881  Accuracy: 0.7999999523162842\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss: 0.7450873851776123  Accuracy: 0.7000000476837158\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss: 0.6577857136726379  Accuracy: 0.7749999761581421\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss: 0.7555899024009705  Accuracy: 0.7250000238418579\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss: 0.6940380334854126  Accuracy: 0.824999988079071\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 0.7442790269851685  Accuracy: 0.8000000715255737\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss: 0.700381338596344  Accuracy: 0.7250000238418579\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss: 0.629246711730957  Accuracy: 0.7750000357627869\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss: 0.8179243803024292  Accuracy: 0.7749999761581421\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss: 0.6665362119674683  Accuracy: 0.875\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 0.7102428674697876  Accuracy: 0.7999999523162842\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss: 0.7238787412643433  Accuracy: 0.7750000357627869\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss: 0.6348865628242493  Accuracy: 0.8500000238418579\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss: 0.7310954332351685  Accuracy: 0.7749999761581421\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss: 0.6342898607254028  Accuracy: 0.824999988079071\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 0.7528787851333618  Accuracy: 0.7999999523162842\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss: 0.72086501121521  Accuracy: 0.75\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss: 0.5924488306045532  Accuracy: 0.8250000476837158\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss: 0.7160943746566772  Accuracy: 0.7250000238418579\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss: 0.6955044865608215  Accuracy: 0.8500000238418579\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 0.7489249110221863  Accuracy: 0.7749999761581421\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss: 0.7338624000549316  Accuracy: 0.7250000238418579\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss: 0.6021895408630371  Accuracy: 0.7750000357627869\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss: 0.7022051811218262  Accuracy: 0.7749999761581421\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss: 0.6302499771118164  Accuracy: 0.8500000238418579\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 0.7002386450767517  Accuracy: 0.7749999761581421\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss: 0.6776924729347229  Accuracy: 0.8250000476837158\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss: 0.5836955308914185  Accuracy: 0.8250000476837158\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss: 0.6918164491653442  Accuracy: 0.7250000238418579\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss: 0.6051977872848511  Accuracy: 0.9000000357627869\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 0.6813311576843262  Accuracy: 0.7749999761581421\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss: 0.6769447922706604  Accuracy: 0.7250000238418579\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss: 0.5585309863090515  Accuracy: 0.7750000357627869\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss: 0.6802701950073242  Accuracy: 0.7250000238418579\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss: 0.6330714225769043  Accuracy: 0.875\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 0.7033467292785645  Accuracy: 0.7749999761581421\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss: 0.708723783493042  Accuracy: 0.7750000357627869\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss: 0.6280953884124756  Accuracy: 0.824999988079071\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss: 0.6523956656455994  Accuracy: 0.7749999761581421\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss: 0.6290645003318787  Accuracy: 0.875\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 0.7439923286437988  Accuracy: 0.8000000715255737\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss: 0.6811327934265137  Accuracy: 0.800000011920929\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss: 0.5775207877159119  Accuracy: 0.824999988079071\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss: 0.6137106418609619  Accuracy: 0.75\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss: 0.608634889125824  Accuracy: 0.875\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 0.6735010147094727  Accuracy: 0.8250000476837158\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss: 0.6586877107620239  Accuracy: 0.7750000357627869\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss: 0.5698540210723877  Accuracy: 0.8250000476837158\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss: 0.7268081307411194  Accuracy: 0.75\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss: 0.613332986831665  Accuracy: 0.875\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 0.6626255512237549  Accuracy: 0.8000000715255737\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss: 0.6675000786781311  Accuracy: 0.824999988079071\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss: 0.5682872533798218  Accuracy: 0.875\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss: 0.6648656129837036  Accuracy: 0.7749999761581421\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss: 0.573401927947998  Accuracy: 0.875\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 0.6343249082565308  Accuracy: 0.8250000476837158\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss: 0.6182407140731812  Accuracy: 0.8750000596046448\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss: 0.5610108375549316  Accuracy: 0.8499999642372131\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss: 0.6648831367492676  Accuracy: 0.75\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss: 0.6129615902900696  Accuracy: 0.875\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 0.7101210355758667  Accuracy: 0.7999999523162842\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss: 0.6482696533203125  Accuracy: 0.800000011920929\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss: 0.5820777416229248  Accuracy: 0.7749999761581421\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss: 0.6448624730110168  Accuracy: 0.7749999761581421\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss: 0.5784995555877686  Accuracy: 0.875\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 0.6831316947937012  Accuracy: 0.824999988079071\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss: 0.6551389694213867  Accuracy: 0.800000011920929\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss: 0.5542770624160767  Accuracy: 0.7750000357627869\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss: 0.645607054233551  Accuracy: 0.8250000476837158\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss: 0.5876340866088867  Accuracy: 0.8999999761581421\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 0.6773068904876709  Accuracy: 0.800000011920929\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss: 0.6231859922409058  Accuracy: 0.824999988079071\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss: 0.5632785558700562  Accuracy: 0.8500000238418579\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss: 0.6490558385848999  Accuracy: 0.800000011920929\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss: 0.5667940378189087  Accuracy: 0.9000000357627869\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 0.6737438440322876  Accuracy: 0.7749999761581421\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss: 0.634798526763916  Accuracy: 0.8499999642372131\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss: 0.5383480787277222  Accuracy: 0.8499999642372131\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss: 0.61073899269104  Accuracy: 0.824999988079071\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss: 0.5232391357421875  Accuracy: 0.875\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 0.6572322845458984  Accuracy: 0.7749999761581421\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss: 0.5918306112289429  Accuracy: 0.875\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss: 0.4984568953514099  Accuracy: 0.875\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss: 0.567333459854126  Accuracy: 0.7749999761581421\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss: 0.5955728888511658  Accuracy: 0.8500000238418579\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 0.6198824644088745  Accuracy: 0.8250000476837158\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss: 0.5972663164138794  Accuracy: 0.8499999642372131\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss: 0.545509397983551  Accuracy: 0.8500000238418579\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss: 0.637814998626709  Accuracy: 0.75\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss: 0.5649527311325073  Accuracy: 0.8500000238418579\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 0.5868768095970154  Accuracy: 0.8250000476837158\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss: 0.5554423332214355  Accuracy: 0.8500000238418579\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss: 0.5205017328262329  Accuracy: 0.824999988079071\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss: 0.5814539194107056  Accuracy: 0.800000011920929\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss: 0.5816644430160522  Accuracy: 0.875\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 0.6117392182350159  Accuracy: 0.7999999523162842\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss: 0.5659514665603638  Accuracy: 0.875\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss: 0.5546659231185913  Accuracy: 0.875\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss: 0.5823464393615723  Accuracy: 0.8500000238418579\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss: 0.6195489168167114  Accuracy: 0.875\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 0.6554155945777893  Accuracy: 0.7999999523162842\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss: 0.5714291334152222  Accuracy: 0.800000011920929\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss: 0.514080286026001  Accuracy: 0.925000011920929\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss: 0.5897172689437866  Accuracy: 0.75\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss: 0.5460026264190674  Accuracy: 0.875\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 0.554981529712677  Accuracy: 0.7999999523162842\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss: 0.5634796619415283  Accuracy: 0.8999999761581421\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss: 0.49345600605010986  Accuracy: 0.8750000596046448\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss: 0.6373656988143921  Accuracy: 0.75\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss: 0.5355686545372009  Accuracy: 0.8500000238418579\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 0.5922667384147644  Accuracy: 0.7999999523162842\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss: 0.6263180375099182  Accuracy: 0.800000011920929\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss: 0.46471068263053894  Accuracy: 0.9000000357627869\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss: 0.5869492292404175  Accuracy: 0.75\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss: 0.5222798585891724  Accuracy: 0.875\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 0.5789551734924316  Accuracy: 0.7999999523162842\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss: 0.5836320519447327  Accuracy: 0.8500000238418579\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss: 0.4980255365371704  Accuracy: 0.925000011920929\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss: 0.6093240976333618  Accuracy: 0.7749999761581421\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss: 0.5462216138839722  Accuracy: 0.875\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 0.5797140598297119  Accuracy: 0.824999988079071\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss: 0.5497565269470215  Accuracy: 0.8750000596046448\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss: 0.5036051273345947  Accuracy: 0.9000000357627869\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss: 0.5273306369781494  Accuracy: 0.8250000476837158\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss: 0.4988590180873871  Accuracy: 0.875\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 0.5820245146751404  Accuracy: 0.7999999523162842\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss: 0.5714734792709351  Accuracy: 0.875\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss: 0.47786664962768555  Accuracy: 0.925000011920929\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss: 0.5552741885185242  Accuracy: 0.800000011920929\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss: 0.49824488162994385  Accuracy: 0.875\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 0.6300913691520691  Accuracy: 0.7999999523162842\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss: 0.5392513871192932  Accuracy: 0.8750000596046448\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss: 0.4614351689815521  Accuracy: 0.949999988079071\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss: 0.5560354590415955  Accuracy: 0.8500000238418579\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss: 0.5164194107055664  Accuracy: 0.875\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 0.6093074679374695  Accuracy: 0.8250000476837158\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss: 0.5433090329170227  Accuracy: 0.8500000834465027\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss: 0.47623181343078613  Accuracy: 0.9000000357627869\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss: 0.5456979274749756  Accuracy: 0.7749999761581421\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss: 0.49138951301574707  Accuracy: 0.8999999761581421\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 0.5517524480819702  Accuracy: 0.8250000476837158\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss: 0.5013831853866577  Accuracy: 0.875\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss: 0.4740610718727112  Accuracy: 0.9000000357627869\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss: 0.5567876696586609  Accuracy: 0.800000011920929\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss: 0.49834921956062317  Accuracy: 0.8500000238418579\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 0.5776047110557556  Accuracy: 0.7999999523162842\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss: 0.5216009616851807  Accuracy: 0.824999988079071\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss: 0.4869041442871094  Accuracy: 0.8750000596046448\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss: 0.5470088124275208  Accuracy: 0.800000011920929\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss: 0.5117713212966919  Accuracy: 0.8500000238418579\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 0.5605161190032959  Accuracy: 0.7999999523162842\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss: 0.5918833613395691  Accuracy: 0.875\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss: 0.4717021882534027  Accuracy: 0.949999988079071\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss: 0.56577467918396  Accuracy: 0.8000000715255737\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss: 0.4979410171508789  Accuracy: 0.875\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 0.5665677785873413  Accuracy: 0.8250000476837158\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss: 0.5256177186965942  Accuracy: 0.824999988079071\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss: 0.4334830939769745  Accuracy: 0.949999988079071\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss: 0.5656387209892273  Accuracy: 0.7999999523162842\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss: 0.4603649079799652  Accuracy: 0.875\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 0.5836236476898193  Accuracy: 0.7999999523162842\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss: 0.528413712978363  Accuracy: 0.8500000238418579\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss: 0.4627034068107605  Accuracy: 0.925000011920929\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss: 0.5561561584472656  Accuracy: 0.800000011920929\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss: 0.5071883797645569  Accuracy: 0.875\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 0.5803813338279724  Accuracy: 0.7999999523162842\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss: 0.5344058275222778  Accuracy: 0.8750000596046448\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss: 0.4588455557823181  Accuracy: 0.9000000357627869\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss: 0.5063591003417969  Accuracy: 0.824999988079071\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss: 0.49279695749282837  Accuracy: 0.875\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 0.555836021900177  Accuracy: 0.7999999523162842\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss: 0.5227870345115662  Accuracy: 0.8500000238418579\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss: 0.45415782928466797  Accuracy: 0.9000000357627869\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss: 0.5175523161888123  Accuracy: 0.7750000357627869\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss: 0.46856483817100525  Accuracy: 0.875\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 0.5601439476013184  Accuracy: 0.8750000596046448\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss: 0.5010230541229248  Accuracy: 0.9000000357627869\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss: 0.47539979219436646  Accuracy: 0.9000000357627869\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss: 0.5252974033355713  Accuracy: 0.7999999523162842\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss: 0.47053244709968567  Accuracy: 0.9000000357627869\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 0.6395816802978516  Accuracy: 0.800000011920929\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss: 0.4915643334388733  Accuracy: 0.925000011920929\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss: 0.40615028142929077  Accuracy: 0.949999988079071\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss: 0.520693302154541  Accuracy: 0.7749999761581421\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss: 0.4773341715335846  Accuracy: 0.8999999761581421\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 0.527849555015564  Accuracy: 0.8250000476837158\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss: 0.513948380947113  Accuracy: 0.875\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss: 0.3852476477622986  Accuracy: 0.949999988079071\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss: 0.4873175024986267  Accuracy: 0.8500000238418579\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss: 0.48271214962005615  Accuracy: 0.9000000357627869\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 0.5627638697624207  Accuracy: 0.7999999523162842\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss: 0.5359938144683838  Accuracy: 0.8750000596046448\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss: 0.45509999990463257  Accuracy: 0.949999988079071\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss: 0.48172295093536377  Accuracy: 0.8250000476837158\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss: 0.45888713002204895  Accuracy: 0.8999999761581421\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 0.5367531180381775  Accuracy: 0.8250000476837158\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss: 0.4797091484069824  Accuracy: 0.9000000357627869\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss: 0.4335055649280548  Accuracy: 0.925000011920929\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss: 0.4860774278640747  Accuracy: 0.875\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss: 0.46437644958496094  Accuracy: 0.8750000596046448\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 0.5350688099861145  Accuracy: 0.824999988079071\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss: 0.49346667528152466  Accuracy: 0.9000000357627869\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss: 0.41470903158187866  Accuracy: 0.9749999642372131\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss: 0.5534900426864624  Accuracy: 0.7749999761581421\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss: 0.3994062840938568  Accuracy: 0.925000011920929\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 0.4932031035423279  Accuracy: 0.8500000238418579\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss: 0.5124200582504272  Accuracy: 0.9000000357627869\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss: 0.4132407009601593  Accuracy: 0.925000011920929\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss: 0.49130064249038696  Accuracy: 0.8250000476837158\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss: 0.4320776164531708  Accuracy: 0.8999999761581421\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 0.52849942445755  Accuracy: 0.7999999523162842\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss: 0.4536125659942627  Accuracy: 0.9000000357627869\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss: 0.4199475646018982  Accuracy: 0.949999988079071\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss: 0.49114957451820374  Accuracy: 0.8250000476837158\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss: 0.44638580083847046  Accuracy: 0.8999999761581421\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 0.5138638019561768  Accuracy: 0.8250000476837158\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss: 0.4713696241378784  Accuracy: 0.925000011920929\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss: 0.3991614580154419  Accuracy: 0.925000011920929\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss: 0.4725037217140198  Accuracy: 0.8500000238418579\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss: 0.4475167691707611  Accuracy: 0.875\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 0.5074122548103333  Accuracy: 0.875\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss: 0.5103331208229065  Accuracy: 0.8750000596046448\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss: 0.43124815821647644  Accuracy: 0.925000011920929\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss: 0.4807467460632324  Accuracy: 0.8250000476837158\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss: 0.4688108563423157  Accuracy: 0.9000000357627869\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 0.5268018245697021  Accuracy: 0.7749999761581421\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss: 0.46008867025375366  Accuracy: 0.8500000238418579\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss: 0.4082874059677124  Accuracy: 0.949999988079071\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss: 0.43556106090545654  Accuracy: 0.9000000357627869\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss: 0.4480403661727905  Accuracy: 0.8999999761581421\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 0.5017156600952148  Accuracy: 0.8500000238418579\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss: 0.4532700777053833  Accuracy: 0.925000011920929\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss: 0.41962140798568726  Accuracy: 0.8750000596046448\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss: 0.4509429931640625  Accuracy: 0.824999988079071\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss: 0.4453253448009491  Accuracy: 0.9000000357627869\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 0.5327025651931763  Accuracy: 0.875\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss: 0.4434005618095398  Accuracy: 0.925000011920929\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss: 0.445281445980072  Accuracy: 0.949999988079071\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss: 0.4704941213130951  Accuracy: 0.8250000476837158\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss: 0.4893897771835327  Accuracy: 0.9000000357627869\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 0.47670358419418335  Accuracy: 0.875\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss: 0.4707574248313904  Accuracy: 0.925000011920929\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss: 0.42232057452201843  Accuracy: 0.925000011920929\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss: 0.44672122597694397  Accuracy: 0.8750000596046448\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss: 0.4530647397041321  Accuracy: 0.925000011920929\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 0.5361612439155579  Accuracy: 0.824999988079071\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss: 0.4819130301475525  Accuracy: 0.925000011920929\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss: 0.40725865960121155  Accuracy: 0.925000011920929\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss: 0.4360653758049011  Accuracy: 0.925000011920929\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss: 0.4354846477508545  Accuracy: 0.925000011920929\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 0.585614800453186  Accuracy: 0.8000000715255737\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss: 0.47769877314567566  Accuracy: 0.9000000357627869\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss: 0.42399775981903076  Accuracy: 0.8750000596046448\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss: 0.4432288408279419  Accuracy: 0.9000000357627869\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss: 0.4361962676048279  Accuracy: 0.9000000357627869\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 0.4948970377445221  Accuracy: 0.8500000238418579\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss: 0.45406121015548706  Accuracy: 0.8750000596046448\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss: 0.42105138301849365  Accuracy: 0.925000011920929\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss: 0.4741789400577545  Accuracy: 0.8499999642372131\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss: 0.4368743300437927  Accuracy: 0.925000011920929\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 0.5003077387809753  Accuracy: 0.7999999523162842\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss: 0.45168381929397583  Accuracy: 0.925000011920929\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss: 0.4417693614959717  Accuracy: 0.925000011920929\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss: 0.4557325541973114  Accuracy: 0.824999988079071\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss: 0.4122638702392578  Accuracy: 0.875\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 0.5037797689437866  Accuracy: 0.8500000238418579\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss: 0.4668366014957428  Accuracy: 0.925000011920929\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss: 0.4012939929962158  Accuracy: 0.9749999642372131\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss: 0.5334030985832214  Accuracy: 0.824999988079071\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss: 0.427778959274292  Accuracy: 0.875\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 0.47177428007125854  Accuracy: 0.8500000238418579\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss: 0.44412776827812195  Accuracy: 0.925000011920929\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss: 0.433327853679657  Accuracy: 0.925000011920929\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss: 0.46019139885902405  Accuracy: 0.875\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss: 0.40195217728614807  Accuracy: 0.925000011920929\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 0.5016571879386902  Accuracy: 0.875\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss: 0.4719041585922241  Accuracy: 0.8750000596046448\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss: 0.40708351135253906  Accuracy: 0.949999988079071\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss: 0.4350295662879944  Accuracy: 0.9000000357627869\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss: 0.43782514333724976  Accuracy: 0.9000000357627869\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 0.4755593538284302  Accuracy: 0.8500000238418579\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss: 0.46002277731895447  Accuracy: 0.9000000357627869\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss: 0.43686342239379883  Accuracy: 0.925000011920929\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss: 0.4528343081474304  Accuracy: 0.8250000476837158\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss: 0.4422810673713684  Accuracy: 0.949999988079071\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 0.4636343717575073  Accuracy: 0.824999988079071\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss: 0.4477485418319702  Accuracy: 0.8750000596046448\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss: 0.4259894788265228  Accuracy: 0.949999988079071\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss: 0.3997395932674408  Accuracy: 0.8750000596046448\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss: 0.44356387853622437  Accuracy: 0.8999999761581421\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 0.4360564649105072  Accuracy: 0.8750000596046448\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss: 0.43425658345222473  Accuracy: 0.925000011920929\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss: 0.39660847187042236  Accuracy: 0.9749999642372131\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss: 0.41853058338165283  Accuracy: 0.925000011920929\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss: 0.42736130952835083  Accuracy: 0.925000011920929\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 0.4284200668334961  Accuracy: 0.9000000357627869\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss: 0.4133313000202179  Accuracy: 0.925000011920929\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss: 0.3990733325481415  Accuracy: 0.9749999642372131\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss: 0.44739168882369995  Accuracy: 0.8750000596046448\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss: 0.42141634225845337  Accuracy: 0.925000011920929\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 0.46514585614204407  Accuracy: 0.875\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss: 0.4361332356929779  Accuracy: 0.9000000357627869\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss: 0.3624825179576874  Accuracy: 0.925000011920929\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss: 0.44200295209884644  Accuracy: 0.8250000476837158\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss: 0.41490626335144043  Accuracy: 0.949999988079071\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 0.44696712493896484  Accuracy: 0.8500000238418579\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss: 0.4214779734611511  Accuracy: 0.925000011920929\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss: 0.4066413938999176  Accuracy: 0.949999988079071\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss: 0.4176161587238312  Accuracy: 0.9000000357627869\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss: 0.43653205037117004  Accuracy: 0.8999999761581421\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 0.4674310088157654  Accuracy: 0.8750000596046448\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss: 0.4181707501411438  Accuracy: 0.925000011920929\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss: 0.42220258712768555  Accuracy: 0.925000011920929\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss: 0.45675647258758545  Accuracy: 0.8500000238418579\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss: 0.4327235221862793  Accuracy: 0.949999988079071\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 0.444960355758667  Accuracy: 0.8750000596046448\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss: 0.4240715503692627  Accuracy: 0.8750000596046448\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss: 0.39079946279525757  Accuracy: 0.9749999642372131\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss: 0.44300034642219543  Accuracy: 0.8500000238418579\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss: 0.44980841875076294  Accuracy: 0.8999999761581421\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.7051028481012658\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f16dc0eb4a8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
