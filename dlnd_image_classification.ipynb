{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [00:16, 10.6MB/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb25dd13a90>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return (x-x.min())/(x.max()-x.min())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return np.eye(10)[x]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, (None, *image_shape), name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, [None,n_classes], name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32,None, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    shape = x_tensor.get_shape().as_list()\n",
    "    size = (conv_ksize[0], conv_ksize[1], shape[3], conv_num_outputs)\n",
    "    W = tf.Variable(tf.truncated_normal(size, 0, (1/ sqrt(shape[1]*shape[2]*shape[3]))))\n",
    "    B = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    tensor = tf.nn.conv2d(x_tensor, W, strides=[1, conv_strides[0], conv_strides[1], 1], padding='SAME')\n",
    "    tensor = tf.nn.bias_add(tensor, B)\n",
    "    tensor = tf.nn.relu(tensor)\n",
    "    tensor = tf.nn.max_pool(tensor, ksize=[1, pool_ksize[0], pool_ksize[1], 1], strides=[1, pool_strides[0], pool_strides[1], 1], padding='SAME')\n",
    "    return tensor\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.reshape(x_tensor, [-1, x_tensor.shape[1].value * x_tensor.shape[2].value* x_tensor.shape[3].value])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    weight= tf.Variable(tf.truncated_normal([x_tensor.shape[1].value, num_outputs], mean=0.0, stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    \n",
    "    return tf.nn.bias_add(tf.matmul(x_tensor, weight), bias)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    weight= tf.Variable(tf.truncated_normal([x_tensor.shape[1].value, num_outputs], mean=0.0, stddev=0.01))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    return tf.nn.bias_add(tf.matmul(x_tensor, weight), bias)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv1 = conv2d_maxpool(x, 20, [5,5], [1,1], [2,2], [2,2])\n",
    "    conv2 = conv2d_maxpool(conv1, 50, conv_ksize=[3,3], conv_strides=[1,1], pool_ksize=[2,2], pool_strides=[2,2])\n",
    "    conv2 = tf.nn.dropout(conv2, keep_prob)\n",
    "    \n",
    "    conv_out = conv2\n",
    "    \n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    fl0 = flatten(conv_out)\n",
    "    \n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    fl1 = fully_conn(fl0, 100)\n",
    "    fl1 = tf.nn.relu(fl1)\n",
    "    fl1 = tf.nn.dropout(fl1, keep_prob)\n",
    "    \n",
    "    fl2 = fully_conn(fl1, 50)\n",
    "    fl2= tf.nn.relu(fl2)\n",
    "    fl2 = tf.nn.dropout(fl2, keep_prob+0.2)\n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    out = output(fl2,10)\n",
    "    \n",
    "    \n",
    "    # TODO: return output\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer,feed_dict={x:feature_batch, y:label_batch, keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    loss = session.run(cost, feed_dict={x:feature_batch, y: label_batch, keep_prob:1.0 })\n",
    "    valid_acc =session.run(accuracy, feed_dict={x:feature_batch, y: label_batch, keep_prob:1.0})\n",
    "    print('Loss: {}  Accuracy: {}'.format(loss, valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 2.268326997756958  Accuracy: 0.125\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 2.208530902862549  Accuracy: 0.20000000298023224\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 2.1626830101013184  Accuracy: 0.20000000298023224\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 2.1039483547210693  Accuracy: 0.20000000298023224\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 2.065370559692383  Accuracy: 0.2750000059604645\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 2.0012571811676025  Accuracy: 0.375\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 1.9912889003753662  Accuracy: 0.3999999761581421\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 1.930774211883545  Accuracy: 0.4000000059604645\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 1.8778553009033203  Accuracy: 0.3500000238418579\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 1.8471615314483643  Accuracy: 0.3500000238418579\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 1.8555960655212402  Accuracy: 0.375\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 1.7952688932418823  Accuracy: 0.42499998211860657\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 1.7396931648254395  Accuracy: 0.375\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 1.7224547863006592  Accuracy: 0.3500000238418579\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 1.7119405269622803  Accuracy: 0.42499998211860657\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 1.6804530620574951  Accuracy: 0.4000000059604645\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 1.6412353515625  Accuracy: 0.44999998807907104\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 1.495839238166809  Accuracy: 0.5\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 1.535882592201233  Accuracy: 0.42499998211860657\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 1.4639972448349  Accuracy: 0.4749999940395355\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 1.386703610420227  Accuracy: 0.5\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 1.3646981716156006  Accuracy: 0.4749999940395355\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 1.3108680248260498  Accuracy: 0.4749999940395355\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 1.3709075450897217  Accuracy: 0.5\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 1.245418906211853  Accuracy: 0.4749999940395355\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 1.2627453804016113  Accuracy: 0.5249999761581421\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 1.2749229669570923  Accuracy: 0.550000011920929\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 1.2335731983184814  Accuracy: 0.5750000476837158\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 1.2034740447998047  Accuracy: 0.550000011920929\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 1.2114918231964111  Accuracy: 0.5750000476837158\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 1.2162768840789795  Accuracy: 0.5750000476837158\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 1.1208515167236328  Accuracy: 0.625\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 1.0901870727539062  Accuracy: 0.6000000238418579\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 1.0453431606292725  Accuracy: 0.625\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 1.0244446992874146  Accuracy: 0.7250000238418579\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 1.0368386507034302  Accuracy: 0.675000011920929\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 0.9929209351539612  Accuracy: 0.675000011920929\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 0.958123505115509  Accuracy: 0.675000011920929\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 0.9187564253807068  Accuracy: 0.7250000238418579\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 0.9195629954338074  Accuracy: 0.7000000476837158\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 0.9459291696548462  Accuracy: 0.675000011920929\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 0.8869509100914001  Accuracy: 0.7250000238418579\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 0.8430415391921997  Accuracy: 0.7000000476837158\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 0.8545835018157959  Accuracy: 0.6500000357627869\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 0.8830217123031616  Accuracy: 0.675000011920929\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 0.8514242172241211  Accuracy: 0.7250000238418579\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 0.8285001516342163  Accuracy: 0.7250000238418579\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 0.8285244107246399  Accuracy: 0.75\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 0.7634793519973755  Accuracy: 0.7000000476837158\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 0.7589956521987915  Accuracy: 0.75\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 0.7623107433319092  Accuracy: 0.75\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 0.7251617312431335  Accuracy: 0.7749999761581421\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 0.7067357301712036  Accuracy: 0.7749999761581421\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 0.7055052518844604  Accuracy: 0.7749999761581421\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 0.6940995454788208  Accuracy: 0.7999999523162842\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 0.6936371922492981  Accuracy: 0.8000000715255737\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 0.7078362703323364  Accuracy: 0.75\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 0.6744713187217712  Accuracy: 0.800000011920929\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 0.6640273332595825  Accuracy: 0.8250000476837158\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 0.6324827671051025  Accuracy: 0.7749999761581421\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 0.6279586553573608  Accuracy: 0.7749999761581421\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 0.6607778668403625  Accuracy: 0.7999999523162842\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 0.6352750062942505  Accuracy: 0.8250000476837158\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 0.615355908870697  Accuracy: 0.8250000476837158\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 0.5950947999954224  Accuracy: 0.8500000238418579\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 0.6058317422866821  Accuracy: 0.8500000238418579\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 0.6402034163475037  Accuracy: 0.800000011920929\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 0.5940229892730713  Accuracy: 0.8500000834465027\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 0.5354049801826477  Accuracy: 0.8750000596046448\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 0.5637300610542297  Accuracy: 0.8500000238418579\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 0.5839259624481201  Accuracy: 0.8500000238418579\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 0.5419413447380066  Accuracy: 0.8500000238418579\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 0.5335173010826111  Accuracy: 0.8500000238418579\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 0.5330908298492432  Accuracy: 0.8750000596046448\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 0.5341019034385681  Accuracy: 0.8750000596046448\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 0.5139391422271729  Accuracy: 0.875\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 0.5134570598602295  Accuracy: 0.8750000596046448\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 0.5192826986312866  Accuracy: 0.8750000596046448\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 0.4744469225406647  Accuracy: 0.8750000596046448\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 0.5148933529853821  Accuracy: 0.8500000238418579\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 0.5147742629051208  Accuracy: 0.8500000238418579\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 0.5262293219566345  Accuracy: 0.8250000476837158\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 0.45890820026397705  Accuracy: 0.9000000357627869\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 0.4828585982322693  Accuracy: 0.925000011920929\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 0.48156264424324036  Accuracy: 0.9000000357627869\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 0.42924830317497253  Accuracy: 0.8750000596046448\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 0.4577413499355316  Accuracy: 0.8750000596046448\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 0.39922139048576355  Accuracy: 0.925000011920929\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 0.4326520562171936  Accuracy: 0.925000011920929\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 0.4272536635398865  Accuracy: 0.925000011920929\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 0.4310108423233032  Accuracy: 0.8500000834465027\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 0.39872196316719055  Accuracy: 0.925000011920929\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 0.40256065130233765  Accuracy: 0.8750000596046448\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 0.41144993901252747  Accuracy: 0.925000011920929\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 0.4197466969490051  Accuracy: 0.8750000596046448\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 0.3720635175704956  Accuracy: 0.9000000357627869\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 0.4027303159236908  Accuracy: 0.925000011920929\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 0.3949185609817505  Accuracy: 0.8750000596046448\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 0.36128777265548706  Accuracy: 0.925000011920929\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 0.367600679397583  Accuracy: 0.925000011920929\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 2.2407495975494385  Accuracy: 0.10000000149011612\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss: 2.069087505340576  Accuracy: 0.32500001788139343\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss: 1.8168511390686035  Accuracy: 0.25\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss: 1.8788169622421265  Accuracy: 0.30000001192092896\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss: 2.014150619506836  Accuracy: 0.2750000059604645\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 2.155445098876953  Accuracy: 0.30000001192092896\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss: 1.8757742643356323  Accuracy: 0.32500001788139343\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss: 1.576603651046753  Accuracy: 0.42499998211860657\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss: 1.6962814331054688  Accuracy: 0.3750000298023224\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss: 1.7440747022628784  Accuracy: 0.3500000238418579\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 1.880205750465393  Accuracy: 0.3999999761581421\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss: 1.7220263481140137  Accuracy: 0.42500001192092896\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss: 1.3569676876068115  Accuracy: 0.42500001192092896\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss: 1.5655268430709839  Accuracy: 0.45000001788139343\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss: 1.6038286685943604  Accuracy: 0.42500001192092896\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 1.7956050634384155  Accuracy: 0.42500001192092896\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss: 1.5300813913345337  Accuracy: 0.5\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss: 1.2526285648345947  Accuracy: 0.5\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss: 1.4761356115341187  Accuracy: 0.4750000238418579\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss: 1.5676349401474  Accuracy: 0.45000001788139343\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 1.6130070686340332  Accuracy: 0.42499998211860657\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss: 1.49021577835083  Accuracy: 0.42499998211860657\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss: 1.1300631761550903  Accuracy: 0.5250000357627869\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss: 1.3988564014434814  Accuracy: 0.4750000238418579\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss: 1.5329532623291016  Accuracy: 0.42500001192092896\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 1.5551068782806396  Accuracy: 0.4749999940395355\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss: 1.3578611612319946  Accuracy: 0.4749999940395355\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss: 1.0988590717315674  Accuracy: 0.5999999642372131\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss: 1.3520221710205078  Accuracy: 0.4750000238418579\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss: 1.423567295074463  Accuracy: 0.42500001192092896\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 1.4595640897750854  Accuracy: 0.5250000357627869\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss: 1.2894840240478516  Accuracy: 0.42500001192092896\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss: 1.036316156387329  Accuracy: 0.625\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss: 1.2222868204116821  Accuracy: 0.550000011920929\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss: 1.3288687467575073  Accuracy: 0.5249999761581421\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 1.4216713905334473  Accuracy: 0.5750000476837158\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss: 1.2065156698226929  Accuracy: 0.44999998807907104\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss: 1.000073790550232  Accuracy: 0.5750000476837158\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss: 1.1937850713729858  Accuracy: 0.550000011920929\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss: 1.2997949123382568  Accuracy: 0.550000011920929\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 1.3345861434936523  Accuracy: 0.5750000476837158\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss: 1.1349985599517822  Accuracy: 0.44999998807907104\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss: 0.9847784042358398  Accuracy: 0.5750000476837158\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss: 1.1771984100341797  Accuracy: 0.6000000238418579\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss: 1.202742576599121  Accuracy: 0.6250000596046448\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 1.2548460960388184  Accuracy: 0.550000011920929\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss: 1.1219254732131958  Accuracy: 0.5\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss: 0.9660706520080566  Accuracy: 0.625\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss: 1.130518913269043  Accuracy: 0.5750000476837158\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss: 1.1853832006454468  Accuracy: 0.5250000357627869\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 1.2452661991119385  Accuracy: 0.6000000238418579\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss: 1.0447204113006592  Accuracy: 0.6000000238418579\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss: 0.9358723759651184  Accuracy: 0.675000011920929\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss: 1.092911720275879  Accuracy: 0.625\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss: 1.0826034545898438  Accuracy: 0.6499999761581421\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 1.216745138168335  Accuracy: 0.5750000476837158\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss: 1.0075386762619019  Accuracy: 0.625\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss: 0.9037131667137146  Accuracy: 0.7000000476837158\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss: 1.024472713470459  Accuracy: 0.625\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss: 1.0956350564956665  Accuracy: 0.6000000238418579\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 1.1339809894561768  Accuracy: 0.6500000357627869\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss: 1.0202174186706543  Accuracy: 0.574999988079071\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss: 0.8478943109512329  Accuracy: 0.675000011920929\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss: 1.015660285949707  Accuracy: 0.5750000476837158\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss: 1.0614936351776123  Accuracy: 0.7000000476837158\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 1.0716530084609985  Accuracy: 0.6500000357627869\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss: 0.9809945821762085  Accuracy: 0.6500000357627869\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss: 0.8421981334686279  Accuracy: 0.7250000238418579\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss: 0.9713417291641235  Accuracy: 0.6499999761581421\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss: 1.0233097076416016  Accuracy: 0.7000000476837158\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 1.0171693563461304  Accuracy: 0.625\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss: 0.8991389870643616  Accuracy: 0.550000011920929\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss: 0.7834067344665527  Accuracy: 0.7250000238418579\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss: 0.9789330959320068  Accuracy: 0.6499999761581421\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss: 1.0128569602966309  Accuracy: 0.7000000476837158\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 1.0123449563980103  Accuracy: 0.675000011920929\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss: 0.8956029415130615  Accuracy: 0.5750000476837158\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss: 0.7993365526199341  Accuracy: 0.7750000357627869\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss: 0.9699227809906006  Accuracy: 0.625\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss: 0.9794538617134094  Accuracy: 0.7250000238418579\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 1.0093059539794922  Accuracy: 0.675000011920929\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss: 0.9196997284889221  Accuracy: 0.7000000476837158\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss: 0.7920852303504944  Accuracy: 0.699999988079071\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss: 0.9241796135902405  Accuracy: 0.6000000238418579\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss: 0.9302573800086975  Accuracy: 0.800000011920929\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 0.9813165068626404  Accuracy: 0.6500000357627869\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss: 0.8250115513801575  Accuracy: 0.6000000238418579\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss: 0.7325782775878906  Accuracy: 0.7749999761581421\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss: 0.9517008066177368  Accuracy: 0.6499999761581421\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss: 0.9134014248847961  Accuracy: 0.7000000476837158\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 0.9128022193908691  Accuracy: 0.6500000357627869\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss: 0.8414250612258911  Accuracy: 0.6499999761581421\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss: 0.6945692300796509  Accuracy: 0.75\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss: 0.8487385511398315  Accuracy: 0.6749999523162842\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss: 0.8678162097930908  Accuracy: 0.7749999761581421\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 0.9510974287986755  Accuracy: 0.7000000476837158\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss: 0.7846347689628601  Accuracy: 0.6750000715255737\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss: 0.7126110196113586  Accuracy: 0.7250000238418579\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss: 0.8700204491615295  Accuracy: 0.6499999761581421\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss: 0.8385509848594666  Accuracy: 0.75\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 0.8837878704071045  Accuracy: 0.7250000238418579\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss: 0.8630751967430115  Accuracy: 0.6499999761581421\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss: 0.6605662107467651  Accuracy: 0.7999999523162842\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss: 0.814072847366333  Accuracy: 0.7000000476837158\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss: 0.8595413565635681  Accuracy: 0.7000000476837158\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 0.8487583994865417  Accuracy: 0.75\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss: 0.7557514905929565  Accuracy: 0.6500000357627869\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss: 0.6428171396255493  Accuracy: 0.800000011920929\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss: 0.84355229139328  Accuracy: 0.7250000238418579\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss: 0.7992513179779053  Accuracy: 0.8500000238418579\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 0.8489986658096313  Accuracy: 0.7250000238418579\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss: 0.7877410650253296  Accuracy: 0.7000000476837158\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss: 0.635542094707489  Accuracy: 0.800000011920929\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss: 0.8075373768806458  Accuracy: 0.75\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss: 0.7879190444946289  Accuracy: 0.75\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 0.8430177569389343  Accuracy: 0.7250000238418579\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss: 0.7140355706214905  Accuracy: 0.7000000476837158\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss: 0.6623533368110657  Accuracy: 0.7500000596046448\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss: 0.7890381813049316  Accuracy: 0.75\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss: 0.7662026286125183  Accuracy: 0.7999999523162842\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 0.7773386836051941  Accuracy: 0.7749999761581421\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss: 0.76423579454422  Accuracy: 0.7000000476837158\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss: 0.6194362640380859  Accuracy: 0.7749999761581421\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss: 0.804171085357666  Accuracy: 0.75\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss: 0.8007510900497437  Accuracy: 0.8500000238418579\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 0.7622886896133423  Accuracy: 0.7250000238418579\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss: 0.6699207425117493  Accuracy: 0.800000011920929\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss: 0.603635847568512  Accuracy: 0.7750000357627869\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss: 0.7853897213935852  Accuracy: 0.6749999523162842\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss: 0.7486754655838013  Accuracy: 0.8250000476837158\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 0.7700471878051758  Accuracy: 0.7250000238418579\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss: 0.6947184801101685  Accuracy: 0.7250000238418579\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss: 0.5798087120056152  Accuracy: 0.8250000476837158\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss: 0.7181607484817505  Accuracy: 0.8250000476837158\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss: 0.7067927122116089  Accuracy: 0.7999999523162842\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 0.7648371458053589  Accuracy: 0.75\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss: 0.6709127426147461  Accuracy: 0.75\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss: 0.596663773059845  Accuracy: 0.800000011920929\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss: 0.7498250007629395  Accuracy: 0.75\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss: 0.7151209115982056  Accuracy: 0.8750000596046448\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 0.778498649597168  Accuracy: 0.75\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss: 0.6605929136276245  Accuracy: 0.75\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss: 0.6001121401786804  Accuracy: 0.8499999642372131\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss: 0.7109417915344238  Accuracy: 0.7749999761581421\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss: 0.6633126735687256  Accuracy: 0.8750000596046448\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 0.7046401500701904  Accuracy: 0.7749999761581421\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss: 0.6824194192886353  Accuracy: 0.7250000238418579\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss: 0.5940694808959961  Accuracy: 0.8500000238418579\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss: 0.6595271825790405  Accuracy: 0.75\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss: 0.6978391408920288  Accuracy: 0.8250000476837158\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 0.7055547833442688  Accuracy: 0.7749999761581421\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss: 0.6730473041534424  Accuracy: 0.75\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss: 0.5680810213088989  Accuracy: 0.8750000596046448\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss: 0.6952897906303406  Accuracy: 0.7749999761581421\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss: 0.7029041647911072  Accuracy: 0.8500000238418579\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 0.7013546824455261  Accuracy: 0.8000000715255737\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss: 0.6112311482429504  Accuracy: 0.7749999761581421\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss: 0.5365984439849854  Accuracy: 0.925000011920929\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss: 0.6857649087905884  Accuracy: 0.800000011920929\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss: 0.6601964831352234  Accuracy: 0.8500000238418579\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 0.6553847789764404  Accuracy: 0.7749999761581421\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss: 0.6318067312240601  Accuracy: 0.8250000476837158\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss: 0.5618765354156494  Accuracy: 0.9000000357627869\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss: 0.6597921848297119  Accuracy: 0.7749999761581421\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss: 0.6681101322174072  Accuracy: 0.8750000596046448\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 0.6899712681770325  Accuracy: 0.8000000715255737\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss: 0.6421152353286743  Accuracy: 0.7500000596046448\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss: 0.5416947603225708  Accuracy: 0.949999988079071\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss: 0.6663821339607239  Accuracy: 0.7250000238418579\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss: 0.5809061527252197  Accuracy: 0.8750000596046448\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 0.6982719302177429  Accuracy: 0.8000000715255737\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss: 0.6026504039764404  Accuracy: 0.800000011920929\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss: 0.5307466387748718  Accuracy: 0.925000011920929\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss: 0.6612953543663025  Accuracy: 0.7999999523162842\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss: 0.6087226271629333  Accuracy: 0.9000000357627869\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 0.6904184818267822  Accuracy: 0.8000000715255737\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss: 0.5805404186248779  Accuracy: 0.8250000476837158\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss: 0.4934183955192566  Accuracy: 0.925000011920929\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss: 0.6638121604919434  Accuracy: 0.75\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss: 0.5951093435287476  Accuracy: 0.875\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 0.6951326131820679  Accuracy: 0.8000000715255737\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss: 0.6218007802963257  Accuracy: 0.8500000238418579\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss: 0.5482319593429565  Accuracy: 0.8750000596046448\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss: 0.6318246722221375  Accuracy: 0.7999999523162842\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss: 0.5869120359420776  Accuracy: 0.8750000596046448\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 0.6709219217300415  Accuracy: 0.8250000476837158\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss: 0.5864244699478149  Accuracy: 0.8000000715255737\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss: 0.5050829648971558  Accuracy: 0.925000011920929\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss: 0.6145772337913513  Accuracy: 0.8250000476837158\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss: 0.5597181916236877  Accuracy: 0.875\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 0.6741799116134644  Accuracy: 0.8000000715255737\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss: 0.6021136045455933  Accuracy: 0.7749999761581421\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss: 0.5046072006225586  Accuracy: 0.9000000357627869\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss: 0.6049880981445312  Accuracy: 0.75\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss: 0.6079317927360535  Accuracy: 0.9000000357627869\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 0.6070348620414734  Accuracy: 0.8000000715255737\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss: 0.578606128692627  Accuracy: 0.8500000834465027\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss: 0.4966207444667816  Accuracy: 0.9749999642372131\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss: 0.6523605585098267  Accuracy: 0.75\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss: 0.5516743659973145  Accuracy: 0.8750000596046448\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 0.6131858825683594  Accuracy: 0.8000000715255737\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss: 0.5625725984573364  Accuracy: 0.824999988079071\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss: 0.483043372631073  Accuracy: 0.9749999642372131\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss: 0.5926914215087891  Accuracy: 0.7999999523162842\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss: 0.5989165306091309  Accuracy: 0.8500000834465027\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 0.6549594402313232  Accuracy: 0.8000000715255737\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss: 0.5505747199058533  Accuracy: 0.8500000238418579\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss: 0.49338364601135254  Accuracy: 0.925000011920929\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss: 0.5890015959739685  Accuracy: 0.75\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss: 0.5753021240234375  Accuracy: 0.8500000238418579\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 0.6038170456886292  Accuracy: 0.8000000715255737\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss: 0.5688196420669556  Accuracy: 0.8250000476837158\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss: 0.46703681349754333  Accuracy: 0.925000011920929\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss: 0.6014531254768372  Accuracy: 0.7750000357627869\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss: 0.5545350313186646  Accuracy: 0.8750000596046448\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 0.553515613079071  Accuracy: 0.8250000476837158\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss: 0.5096043348312378  Accuracy: 0.8750000596046448\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss: 0.4650740325450897  Accuracy: 0.925000011920929\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss: 0.533523678779602  Accuracy: 0.8250000476837158\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss: 0.5458324551582336  Accuracy: 0.9000000357627869\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 0.5814709663391113  Accuracy: 0.8000000715255737\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss: 0.5034164786338806  Accuracy: 0.8250000476837158\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss: 0.4620792865753174  Accuracy: 0.949999988079071\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss: 0.5589136481285095  Accuracy: 0.800000011920929\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss: 0.6013224124908447  Accuracy: 0.8500000238418579\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 0.6229428052902222  Accuracy: 0.8000000715255737\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss: 0.5032331943511963  Accuracy: 0.8500000834465027\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss: 0.48799949884414673  Accuracy: 0.925000011920929\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss: 0.6278180480003357  Accuracy: 0.800000011920929\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss: 0.5442846417427063  Accuracy: 0.9000000357627869\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 0.6133771538734436  Accuracy: 0.8000000715255737\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss: 0.5068066120147705  Accuracy: 0.8750000596046448\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss: 0.4765074849128723  Accuracy: 0.9000000357627869\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss: 0.5180338621139526  Accuracy: 0.8750000596046448\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss: 0.5176214575767517  Accuracy: 0.925000011920929\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 0.6215546131134033  Accuracy: 0.8000000715255737\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss: 0.48788195848464966  Accuracy: 0.9000000357627869\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss: 0.4683508276939392  Accuracy: 0.9000000357627869\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss: 0.5359285473823547  Accuracy: 0.8000000715255737\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss: 0.5325907468795776  Accuracy: 0.8750000596046448\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 0.5688771605491638  Accuracy: 0.8250000476837158\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss: 0.5548800230026245  Accuracy: 0.7750000357627869\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss: 0.4618174433708191  Accuracy: 0.9749999642372131\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss: 0.5433706045150757  Accuracy: 0.8000000715255737\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss: 0.4628949761390686  Accuracy: 0.925000011920929\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 0.5887771844863892  Accuracy: 0.8000000715255737\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss: 0.49181950092315674  Accuracy: 0.9000000357627869\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss: 0.4447938799858093  Accuracy: 0.925000011920929\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss: 0.5702803134918213  Accuracy: 0.7749999761581421\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss: 0.5178685784339905  Accuracy: 0.9000000357627869\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 0.5732035636901855  Accuracy: 0.8250000476837158\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss: 0.49077391624450684  Accuracy: 0.8750000596046448\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss: 0.4486237168312073  Accuracy: 0.925000011920929\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss: 0.5216631293296814  Accuracy: 0.875\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss: 0.5119935274124146  Accuracy: 0.9000000357627869\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 0.5449042916297913  Accuracy: 0.8500000238418579\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss: 0.5037084817886353  Accuracy: 0.8500000834465027\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss: 0.4822689890861511  Accuracy: 0.949999988079071\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss: 0.5172847509384155  Accuracy: 0.824999988079071\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss: 0.46731823682785034  Accuracy: 0.925000011920929\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 0.5474737882614136  Accuracy: 0.8250000476837158\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss: 0.5296074748039246  Accuracy: 0.800000011920929\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss: 0.3935115933418274  Accuracy: 0.925000011920929\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss: 0.4795268774032593  Accuracy: 0.8250000476837158\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss: 0.48663586378097534  Accuracy: 0.9000000357627869\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 0.6046468019485474  Accuracy: 0.8000000715255737\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss: 0.515207827091217  Accuracy: 0.8500000834465027\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss: 0.41655510663986206  Accuracy: 0.9000000357627869\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss: 0.5341647863388062  Accuracy: 0.7750000357627869\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss: 0.48860275745391846  Accuracy: 0.9000000357627869\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 0.559048056602478  Accuracy: 0.7749999761581421\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss: 0.5064870119094849  Accuracy: 0.8750000596046448\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss: 0.4340100884437561  Accuracy: 0.949999988079071\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss: 0.5193189382553101  Accuracy: 0.8500000238418579\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss: 0.49869102239608765  Accuracy: 0.925000011920929\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 0.5930033922195435  Accuracy: 0.75\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss: 0.5216821432113647  Accuracy: 0.8250000476837158\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss: 0.4105837047100067  Accuracy: 0.949999988079071\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss: 0.48498761653900146  Accuracy: 0.8500000238418579\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss: 0.4803277850151062  Accuracy: 0.949999988079071\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 0.5215897560119629  Accuracy: 0.8250000476837158\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss: 0.49760007858276367  Accuracy: 0.8500000834465027\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss: 0.41942277550697327  Accuracy: 0.949999988079071\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss: 0.478107213973999  Accuracy: 0.875\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss: 0.4517296552658081  Accuracy: 0.949999988079071\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 0.5570254325866699  Accuracy: 0.7749999761581421\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss: 0.507575273513794  Accuracy: 0.9000000357627869\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss: 0.4117780327796936  Accuracy: 0.9749999642372131\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss: 0.5338666439056396  Accuracy: 0.8750000596046448\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss: 0.45447883009910583  Accuracy: 0.9000000357627869\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 0.5139736533164978  Accuracy: 0.8250000476837158\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss: 0.44384828209877014  Accuracy: 0.8750000596046448\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss: 0.40547412633895874  Accuracy: 0.925000011920929\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss: 0.5289219617843628  Accuracy: 0.8500000238418579\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss: 0.459534615278244  Accuracy: 0.925000011920929\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 0.5304921269416809  Accuracy: 0.8250000476837158\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss: 0.45910558104515076  Accuracy: 0.925000011920929\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss: 0.39354026317596436  Accuracy: 0.9000000357627869\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss: 0.5004187822341919  Accuracy: 0.8250000476837158\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss: 0.4486338496208191  Accuracy: 0.9000000357627869\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 0.5269338488578796  Accuracy: 0.8000000715255737\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss: 0.4701893925666809  Accuracy: 0.9000000357627869\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss: 0.40481066703796387  Accuracy: 0.8750000596046448\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss: 0.44915711879730225  Accuracy: 0.9000000357627869\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss: 0.4387247562408447  Accuracy: 0.925000011920929\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 0.4806364178657532  Accuracy: 0.8250000476837158\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss: 0.42467036843299866  Accuracy: 0.9000000357627869\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss: 0.38302627205848694  Accuracy: 0.925000011920929\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss: 0.49289751052856445  Accuracy: 0.875\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss: 0.4240667521953583  Accuracy: 0.949999988079071\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 0.5210154056549072  Accuracy: 0.8500000238418579\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss: 0.4494049847126007  Accuracy: 0.925000011920929\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss: 0.3951845169067383  Accuracy: 0.9749999642372131\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss: 0.44971391558647156  Accuracy: 0.9000000357627869\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss: 0.4340910017490387  Accuracy: 0.949999988079071\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 0.534433901309967  Accuracy: 0.8500000834465027\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss: 0.4332537353038788  Accuracy: 0.925000011920929\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss: 0.4029642939567566  Accuracy: 0.925000011920929\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss: 0.48283860087394714  Accuracy: 0.8250000476837158\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss: 0.4803650975227356  Accuracy: 0.8999999761581421\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 0.49573811888694763  Accuracy: 0.8750000596046448\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss: 0.4162862300872803  Accuracy: 0.925000011920929\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss: 0.36927664279937744  Accuracy: 0.9749999642372131\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss: 0.47155171632766724  Accuracy: 0.875\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss: 0.4039805829524994  Accuracy: 0.925000011920929\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 0.47456952929496765  Accuracy: 0.8250000476837158\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss: 0.45316991209983826  Accuracy: 0.9000000357627869\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss: 0.36310452222824097  Accuracy: 0.949999988079071\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss: 0.47559428215026855  Accuracy: 0.8500000238418579\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss: 0.40985769033432007  Accuracy: 0.925000011920929\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 0.5115920305252075  Accuracy: 0.8250000476837158\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss: 0.44405099749565125  Accuracy: 0.9000000357627869\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss: 0.4051688313484192  Accuracy: 0.9749999642372131\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss: 0.4498499631881714  Accuracy: 0.8750000596046448\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss: 0.4648936092853546  Accuracy: 0.925000011920929\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 0.5135508179664612  Accuracy: 0.8250000476837158\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss: 0.41312938928604126  Accuracy: 0.9000000357627869\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss: 0.39066606760025024  Accuracy: 0.949999988079071\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss: 0.4579812288284302  Accuracy: 0.8750000596046448\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss: 0.42255663871765137  Accuracy: 0.925000011920929\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 0.4629127085208893  Accuracy: 0.8500000834465027\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss: 0.4344574809074402  Accuracy: 0.925000011920929\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss: 0.4015263319015503  Accuracy: 0.925000011920929\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss: 0.47608184814453125  Accuracy: 0.8500000238418579\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss: 0.4737817347049713  Accuracy: 0.925000011920929\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 0.5800991058349609  Accuracy: 0.8250000476837158\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss: 0.429150253534317  Accuracy: 0.949999988079071\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss: 0.35238370299339294  Accuracy: 0.9749999642372131\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss: 0.4161789119243622  Accuracy: 0.9000000357627869\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss: 0.4421997666358948  Accuracy: 0.925000011920929\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 0.48813050985336304  Accuracy: 0.8500000834465027\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss: 0.44086799025535583  Accuracy: 0.9000000357627869\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss: 0.3830345869064331  Accuracy: 0.9749999642372131\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss: 0.4661426842212677  Accuracy: 0.949999988079071\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss: 0.5004439353942871  Accuracy: 0.925000011920929\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 0.5214780569076538  Accuracy: 0.8500000238418579\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss: 0.4146793484687805  Accuracy: 0.8750000596046448\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss: 0.38113492727279663  Accuracy: 0.9000000357627869\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss: 0.4278004467487335  Accuracy: 0.9000000357627869\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss: 0.4372037351131439  Accuracy: 0.949999988079071\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 0.47364312410354614  Accuracy: 0.8750000596046448\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss: 0.40671613812446594  Accuracy: 0.9000000357627869\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss: 0.3843887448310852  Accuracy: 0.949999988079071\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss: 0.4383828938007355  Accuracy: 0.8500000238418579\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss: 0.3858829140663147  Accuracy: 0.949999988079071\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 0.47803401947021484  Accuracy: 0.8250000476837158\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss: 0.42736825346946716  Accuracy: 0.9000000357627869\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss: 0.3653865456581116  Accuracy: 0.9749999642372131\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss: 0.43965446949005127  Accuracy: 0.8500000238418579\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss: 0.41351473331451416  Accuracy: 0.949999988079071\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 0.5170061588287354  Accuracy: 0.8250000476837158\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss: 0.39117515087127686  Accuracy: 0.925000011920929\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss: 0.36369597911834717  Accuracy: 0.925000011920929\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss: 0.41980916261672974  Accuracy: 0.875\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss: 0.45574262738227844  Accuracy: 0.925000011920929\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 0.4748358428478241  Accuracy: 0.8500000834465027\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss: 0.37971031665802  Accuracy: 0.925000011920929\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss: 0.3438948690891266  Accuracy: 0.949999988079071\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss: 0.4245269000530243  Accuracy: 0.875\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss: 0.43432751297950745  Accuracy: 0.949999988079071\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 0.48032528162002563  Accuracy: 0.8500000238418579\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss: 0.38449540734291077  Accuracy: 0.949999988079071\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss: 0.37229466438293457  Accuracy: 0.949999988079071\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss: 0.43355679512023926  Accuracy: 0.9000000357627869\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss: 0.4142286777496338  Accuracy: 0.949999988079071\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 0.45925939083099365  Accuracy: 0.8750000596046448\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss: 0.376315712928772  Accuracy: 0.949999988079071\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss: 0.3371473550796509  Accuracy: 0.949999988079071\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss: 0.43007326126098633  Accuracy: 0.9000000357627869\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss: 0.44326043128967285  Accuracy: 0.925000011920929\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 0.44055309891700745  Accuracy: 0.8500000834465027\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss: 0.4143601059913635  Accuracy: 0.925000011920929\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss: 0.38401296734809875  Accuracy: 0.949999988079071\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss: 0.3936292827129364  Accuracy: 0.8750000596046448\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss: 0.44862866401672363  Accuracy: 0.925000011920929\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 0.5260477662086487  Accuracy: 0.8250000476837158\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss: 0.3790081739425659  Accuracy: 0.949999988079071\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss: 0.3530743420124054  Accuracy: 0.9749999642372131\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss: 0.43104612827301025  Accuracy: 0.9000000357627869\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss: 0.43299365043640137  Accuracy: 0.949999988079071\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 0.49202263355255127  Accuracy: 0.8500000834465027\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss: 0.3951534628868103  Accuracy: 0.925000011920929\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss: 0.3167509436607361  Accuracy: 0.9749999642372131\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss: 0.3981955945491791  Accuracy: 0.875\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss: 0.4094409942626953  Accuracy: 0.9750000238418579\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 0.4618046283721924  Accuracy: 0.8250000476837158\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss: 0.390134334564209  Accuracy: 0.9000000357627869\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss: 0.3386383652687073  Accuracy: 0.9749999642372131\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss: 0.4002693295478821  Accuracy: 0.925000011920929\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss: 0.4313781261444092  Accuracy: 0.949999988079071\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 0.4982718825340271  Accuracy: 0.9000000357627869\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss: 0.3695233464241028  Accuracy: 0.925000011920929\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss: 0.34098005294799805  Accuracy: 0.949999988079071\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss: 0.4399954676628113  Accuracy: 0.9000000357627869\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss: 0.40989619493484497  Accuracy: 0.949999988079071\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 0.45011135935783386  Accuracy: 0.8250000476837158\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss: 0.3938070237636566  Accuracy: 0.925000011920929\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss: 0.32094806432724  Accuracy: 0.9749999642372131\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss: 0.42528125643730164  Accuracy: 0.9000000357627869\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss: 0.44114720821380615  Accuracy: 0.949999988079071\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 0.4881942868232727  Accuracy: 0.8500000834465027\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss: 0.3644239902496338  Accuracy: 0.949999988079071\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss: 0.31464478373527527  Accuracy: 0.949999988079071\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss: 0.396472692489624  Accuracy: 0.9000000357627869\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss: 0.4165257513523102  Accuracy: 0.925000011920929\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 0.5196034908294678  Accuracy: 0.8750000596046448\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss: 0.38985130190849304  Accuracy: 0.925000011920929\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss: 0.34242820739746094  Accuracy: 0.9749999642372131\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss: 0.4058361053466797  Accuracy: 0.925000011920929\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss: 0.408505916595459  Accuracy: 0.949999988079071\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 0.528192937374115  Accuracy: 0.8250000476837158\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss: 0.37034040689468384  Accuracy: 0.9000000357627869\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss: 0.32473132014274597  Accuracy: 0.949999988079071\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss: 0.4400516152381897  Accuracy: 0.875\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss: 0.3913237750530243  Accuracy: 0.9249999523162842\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 0.5759196281433105  Accuracy: 0.8250000476837158\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss: 0.3565922975540161  Accuracy: 0.949999988079071\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss: 0.32496151328086853  Accuracy: 0.949999988079071\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss: 0.41793981194496155  Accuracy: 0.9000000357627869\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss: 0.3716503381729126  Accuracy: 0.949999988079071\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 0.5174421668052673  Accuracy: 0.9000000357627869\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss: 0.4003094434738159  Accuracy: 0.9000000357627869\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss: 0.3294124901294708  Accuracy: 0.949999988079071\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss: 0.3989947438240051  Accuracy: 0.9000000357627869\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss: 0.3663467466831207  Accuracy: 0.925000011920929\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 0.4584028720855713  Accuracy: 0.8750000596046448\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss: 0.3790633976459503  Accuracy: 0.925000011920929\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss: 0.34440481662750244  Accuracy: 0.949999988079071\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss: 0.3658299446105957  Accuracy: 0.925000011920929\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss: 0.43006041646003723  Accuracy: 0.9000000357627869\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 0.45537546277046204  Accuracy: 0.8500000238418579\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss: 0.41356879472732544  Accuracy: 0.9000000357627869\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss: 0.3164216876029968  Accuracy: 0.9749999642372131\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss: 0.4041340947151184  Accuracy: 0.9000000357627869\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss: 0.4379153251647949  Accuracy: 0.949999988079071\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 0.48249417543411255  Accuracy: 0.8250000476837158\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss: 0.38475874066352844  Accuracy: 0.9000000357627869\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss: 0.32911619544029236  Accuracy: 0.9749999642372131\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss: 0.3895798325538635  Accuracy: 0.9000000357627869\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss: 0.38083672523498535  Accuracy: 0.949999988079071\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 0.4281251132488251  Accuracy: 0.8500000834465027\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss: 0.3901599049568176  Accuracy: 0.949999988079071\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss: 0.2960686683654785  Accuracy: 0.9749999642372131\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss: 0.4035079777240753  Accuracy: 0.925000011920929\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss: 0.35877302289009094  Accuracy: 0.9750000238418579\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 0.4240460991859436  Accuracy: 0.8750000596046448\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss: 0.38979724049568176  Accuracy: 0.9000000357627869\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss: 0.3080118000507355  Accuracy: 0.949999988079071\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss: 0.35793066024780273  Accuracy: 0.9000000357627869\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss: 0.36526429653167725  Accuracy: 0.949999988079071\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 0.487946093082428  Accuracy: 0.8250000476837158\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss: 0.4087384343147278  Accuracy: 0.9000000357627869\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss: 0.30151745676994324  Accuracy: 0.9749999642372131\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss: 0.3484190106391907  Accuracy: 0.925000011920929\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss: 0.3617895543575287  Accuracy: 0.9750000238418579\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 0.4520394802093506  Accuracy: 0.8500000238418579\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss: 0.35720765590667725  Accuracy: 0.9000000357627869\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss: 0.32202741503715515  Accuracy: 0.9749999642372131\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss: 0.3338443636894226  Accuracy: 0.9749999642372131\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss: 0.3896934390068054  Accuracy: 0.925000011920929\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 0.49799346923828125  Accuracy: 0.8500000238418579\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss: 0.39633670449256897  Accuracy: 0.925000011920929\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss: 0.3166663944721222  Accuracy: 0.9749999642372131\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss: 0.39895468950271606  Accuracy: 0.8750000596046448\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss: 0.36684665083885193  Accuracy: 0.949999988079071\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 0.4632011950016022  Accuracy: 0.8500000238418579\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss: 0.368091344833374  Accuracy: 0.925000011920929\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss: 0.3240647315979004  Accuracy: 0.949999988079071\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss: 0.3424139618873596  Accuracy: 0.925000011920929\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss: 0.38683009147644043  Accuracy: 0.949999988079071\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 0.4911814332008362  Accuracy: 0.8250000476837158\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss: 0.3731101155281067  Accuracy: 0.9000000357627869\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss: 0.363290935754776  Accuracy: 0.925000011920929\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss: 0.3451484441757202  Accuracy: 0.949999988079071\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss: 0.39079248905181885  Accuracy: 0.9750000238418579\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 0.4698379635810852  Accuracy: 0.8750000596046448\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss: 0.383028507232666  Accuracy: 0.9000000357627869\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss: 0.34002795815467834  Accuracy: 0.949999988079071\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss: 0.38642120361328125  Accuracy: 0.925000011920929\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss: 0.3838786482810974  Accuracy: 0.9750000238418579\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.7134098101265823\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb20b9ca320>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
